{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8725a71f-02be-4933-973c-d120cbf46f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2302b2fd-7579-46c1-96e4-1ebc5321ea28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from ddpg_torch import Agent\n",
    "# import gym\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "# agent = Agent(alpha=0.000025, beta=0.00025, input_dims=[4], tau=0.001, env=env, batch_size=64, layer1_size=400, layer2_size=300, n_actions=2)\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# score_history = []\n",
    "# for i in range(1000):\n",
    "#     done = False\n",
    "#     score = 0\n",
    "#     observation, info = env.reset()\n",
    "#     while not done:\n",
    "#         action = agent.choose_action(observation)\n",
    "#         new_state, reward, terminated, truncated, step_info = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         agent.remember(observation, action, reward, new_state, int(done))\n",
    "#         agent.learn()\n",
    "#         score += reward\n",
    "#         observation = new_state\n",
    "    \n",
    "#     score_history.append(score)\n",
    "#     print('episode ', i, 'score %.2f' % score\n",
    "#           , '100 game average %.2f' % np.mean(score_history[-100:]))\n",
    "    \n",
    "#     if i % 25 == 0:\n",
    "#         agent.save_models()\n",
    "    \n",
    "#     filename = 'scores.png'\n",
    "#     x = [i+1 for i in range(len(score_history))]\n",
    "#     running_avg = np.zeros(len(score_history))\n",
    "\n",
    "#     import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # after the loop, once score_history is filled:\n",
    "# x = np.arange(1, len(score_history)+1)\n",
    "# running_avg = np.zeros_like(x, dtype=float)\n",
    "\n",
    "# for t in range(len(x)):\n",
    "#     running_avg[t] = np.mean(score_history[max(0, t-99):t+1])\n",
    "\n",
    "# plt.plot(x, score_history, label='Score per episode')\n",
    "# plt.plot(x, running_avg, label='100-episode running average')\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Score')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "27ca6eb4-c54c-48b8-9054-e58528494024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs('tmp/ddpg', exist_ok=True)\n",
    "\n",
    "# # Gym’s checker and NumPy ≥1.25\n",
    "# import numpy as np\n",
    "# np.bool8 = np.bool_\n",
    "\n",
    "# import gymnasium as gym\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from ddpg_torch import Agent\n",
    "\n",
    "# # create continuous-action environment\n",
    "# env = gym.make(\"LunarLanderContinuous-v3\")\n",
    "\n",
    "# # pull dims straight from env\n",
    "# obs_dim    = env.observation_space.shape[0]   # 8\n",
    "# n_actions  = env.action_space.shape[0]        # 2\n",
    "\n",
    "# agent = Agent(\n",
    "#     alpha=2.5e-05, beta=2.5e-04,\n",
    "#     input_dims=[obs_dim], tau=0.001, env=env,\n",
    "#     batch_size=64, layer1_size=400, layer2_size=300,\n",
    "#     n_actions=n_actions\n",
    "# )\n",
    "\n",
    "# # reproducibility\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "# env.reset(seed=0)\n",
    "\n",
    "# score_history = []\n",
    "# for i in range(1000):\n",
    "#     observation, _ = env.reset()\n",
    "#     done = False\n",
    "#     score = 0.0\n",
    "\n",
    "#     while not done:\n",
    "#         # now we can directly use the continuous output\n",
    "#         action = agent.choose_action(observation)  \n",
    "\n",
    "#         new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "\n",
    "#         agent.remember(observation, action, reward, new_state, int(done))\n",
    "#         agent.learn()\n",
    "\n",
    "#         score += reward\n",
    "#         observation = new_state\n",
    "\n",
    "#     score_history.append(score)\n",
    "#     avg100 = np.mean(score_history[-100:])\n",
    "#     print(f\"Episode {i:4d}  Score {score:6.2f}  100-avg {avg100:6.2f}\")\n",
    "\n",
    "#     if i % 25 == 0:\n",
    "#         agent.save_models()\n",
    "\n",
    "# # plot results\n",
    "# x = np.arange(1, len(score_history) + 1)\n",
    "# running_avg = np.zeros_like(x, dtype=float)\n",
    "# for t in range(len(x)):\n",
    "#     running_avg[t] = np.mean(score_history[max(0, t-99):t+1])\n",
    "\n",
    "# plt.plot(x, score_history, label=\"Score per episode\")\n",
    "# plt.plot(x, running_avg, label=\"100-episode running average\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8271bd3e-50e0-48eb-a7df-d5ef21b5853d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install swig\n",
    "%pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd286d3-0362-4e47-b0af-a74461dde0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Farama-Foundation/Gymnasium-Robotics.git\n",
    "%pip uninstall -y mujoco-py\n",
    "%pip install mujoco\n",
    "%pip install gymnasium[robotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd3d613-9b13-4c8b-8f70-8a93a3467048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b028ba6-f527-4d1e-8228-aa13223d1c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "\n",
    "# 1) Create the FetchPush-v4 environment\n",
    "env = gym.make(\"FetchPush-v4\")\n",
    "\n",
    "# 2) Reset the environment to get the initial observation\n",
    "obs, info = env.reset()\n",
    "\n",
    "# 3) Print the observation structure\n",
    "print(\"Observation keys:\", obs.keys())\n",
    "print(\"Sample observation:\")\n",
    "for key, value in obs.items():\n",
    "    print(f\"  {key}: shape={value.shape}, type={type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b85520-5d48-42e7-9c3a-c448eb4f2117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics  # Ensures robotics envs are registered\n",
    "\n",
    "# Print all Fetch-related environments\n",
    "fetch_envs = [env_spec.id for env_spec in gym.registry.values() if \"Fetch\" in env_spec.id]\n",
    "print(fetch_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c827c1-fd0b-4fe6-a74d-76ec953d3d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('tmp/ddpg', exist_ok=True)\n",
    "\n",
    "# Gym’s PassiveEnvChecker fix\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics  # ensures the Fetch* envs are registered\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ddpg_torch import Agent\n",
    "\n",
    "# 1) Create the FetchPush-v4 env\n",
    "env = gym.make(\"FetchPush-v4\")\n",
    "\n",
    "# 2) Combine 'observation' and 'desired_goal' as input\n",
    "obs_space = env.observation_space.spaces['observation']\n",
    "goal_space = env.observation_space.spaces['desired_goal']\n",
    "obs_dim = obs_space.shape[0] + goal_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# 3) Build your DDPG agent\n",
    "agent = Agent(\n",
    "    alpha=1e-4, beta=1e-3,  # ↑ Increased LR for better early learning\n",
    "    input_dims=[obs_dim], tau=0.001, env=env,\n",
    "    batch_size=64, layer1_size=400, layer2_size=300,\n",
    "    n_actions=act_dim\n",
    ")\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "env.reset(seed=0)\n",
    "\n",
    "# 4) Training loop\n",
    "score_history = []\n",
    "success_history = []\n",
    "\n",
    "update_freq = 2\n",
    "warmup_episodes = 10\n",
    "warmup_steps = 1000\n",
    "total_steps = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    obs_dict, _ = env.reset()\n",
    "    agent.noise.reset()\n",
    "    state = np.concatenate([obs_dict['observation'], obs_dict['desired_goal']])\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    step = 0\n",
    "    success = 0\n",
    "\n",
    "    while not done:\n",
    "        # Warm-up exploration (random action)\n",
    "        if i < warmup_episodes:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.choose_action(state)\n",
    "            action += 0.1 * np.random.randn(act_dim)  # Gaussian noise\n",
    "            action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        next_obs_dict, _, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # HER-style reward recomputation\n",
    "        achieved_goal = next_obs_dict[\"achieved_goal\"]\n",
    "        desired_goal = next_obs_dict[\"desired_goal\"]\n",
    "        #reward = env.compute_reward(achieved_goal, desired_goal, info={})\n",
    "        reward = agent._reward_env.compute_reward(achieved_goal, desired_goal, info={})\n",
    "\n",
    "        next_state = np.concatenate([next_obs_dict['observation'], desired_goal])\n",
    "        agent.remember(state, action, reward, next_state, int(done))\n",
    "\n",
    "        # Learn only after sufficient buffer and at intervals\n",
    "        if total_steps > warmup_steps and step % update_freq == 0:\n",
    "            agent.learn()\n",
    "\n",
    "        score += reward\n",
    "        success += info.get(\"is_success\", 0.0)\n",
    "        state = next_state\n",
    "        step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "    score_history.append(score)\n",
    "    success_history.append(success)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    avg_success = np.mean(success_history[-100:]) * 100\n",
    "\n",
    "    print(f\"Episode {i:4d}  Score {score:6.2f}  100-avg {avg_score:6.2f}  Success {avg_success:.1f}%\")\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        agent.save_models()\n",
    "\n",
    "# 5) Plot results\n",
    "x = np.arange(1, len(score_history) + 1)\n",
    "running_avg = np.zeros_like(x, dtype=float)\n",
    "for t in range(len(x)):\n",
    "    running_avg[t] = np.mean(score_history[max(0, t-99):t+1])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(x, score_history, label=\"Score per episode\")\n",
    "plt.plot(x, running_avg, label=\"100-episode running avg\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"DDPG on FetchPush-v4\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
